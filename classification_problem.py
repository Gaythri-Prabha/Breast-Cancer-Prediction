# -*- coding: utf-8 -*-
"""Classification Problem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KpesVOVK-dYUtqzW-PA3IyVCP4ExZdFH

# **1.loading and Preprocessing data**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()

df = pd.DataFrame(cancer.data, columns=cancer.feature_names)
df.head()

df['target'] = cancer.target

df.info()

df.describe()

df.duplicated().sum()

print(df.isnull().sum())  # Checking for missing values

#Feature scaling is necessary for algorithms like SVM and k-NN that are sensitive to different feature scales.
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(df.iloc[:,:-1])
y = df['target']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# **2. Classification Algorithm Implementation **"""

#logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

lr_acc = accuracy_score(y_test, y_pred_lr)#checking for accuracy
print("Logistic Regression Accuracy:", lr_acc)

"""  

*   Logistic Regression is a linear model used for binary classification.

*   Logistic Regression is a simple yet effective linear model for binary
classification



"""

#Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

dt_acc = accuracy_score(y_test, y_pred_dt)#checking for accuracy
print("Decision Tree Classifier Accuracy:", dt_acc)

"""
*   A tree-based model that makes decisions by recursively splitting the data based on feature values.
*  Decision Trees are interpretable and can model complex decision boundaries.


"""

#Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

rf_acc = accuracy_score(y_test, y_pred_rf)#checking for accuracy
print("RandomForestClassifier Accuracy:", rf_acc)

"""

*   An ensemble method that builds multiple decision trees and combines their predictions.
*   Random Forest reduces overfitting by using multiple decision trees.

"""

# Support Vector Machine (SVM)
from sklearn.svm import SVC

svm = SVC(kernel='linear')
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)

svm_acc = accuracy_score(y_test, y_pred_svm)
print(f"SVM Accuracy: {svm_acc:.4f}")

"""


*   SVM finds the optimal hyperplane that separates the two classes (benign vs. malignant).
It maximizes the margin between the closest points from both classes (support vectors).
*  SVM works well with high-dimensional data and finds the optimal hyperplane.

"""

#k-Nearest Neighbors (k-NN)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

knn_acc = accuracy_score(y_test, y_pred_knn)
print(f"k-NN Accuracy: {knn_acc:.4f}")

"""A distance-based classifier that assigns a class based on the majority label of the k nearest neighbors.k-NN is a distance-based algorithm that works well when data is properly scaled.

# **3. Model Comparison**
"""

model_accuracies = {
    "Logistic Regression": lr_acc,
    "Decision Tree": dt_acc,
    "Random Forest": rf_acc,
    "SVM": svm_acc,
    "k-NN": knn_acc
}

# Print results
for model, acc in model_accuracies.items():
    print(f"{model}: {acc:.4f}")

# Best and Worst Models
best_model = max(model_accuracies, key=model_accuracies.get)
worst_model = min(model_accuracies, key=model_accuracies.get)

print(f"Best performing model: {best_model} with {model_accuracies[best_model]:.4f} accuracy")
print(f"Worst performing model: {worst_model} with {model_accuracies[worst_model]:.4f} accuracy")

"""Logistic Regression: 0.9737

Decision Tree: 0.9386

Random Forest: 0.9649

SVM: 0.9561

k-NN: 0.9474

Best performing model: Logistic Regression with 0.9737 accuracy

Worst performing model: Decision Tree with 0.9386 accuracy
"""





